@inproceedings{8461000,
  author    = {Droeschel, David and Behnke, Sven},
  booktitle = {2018 IEEE International Conference on Robotics and Automation (ICRA)},
  title     = {Efficient Continuous-Time SLAM for 3D Lidar-Based Online Mapping},
  year      = {2018},
  volume    = {},
  number    = {},
  pages     = {5000-5007},
  doi       = {10.1109/ICRA.2018.8461000}
}

 @article{Artal_Montiel_Tardos_2015,
  title        = {ORB-SLAM: A Versatile and Accurate Monocular SLAM System},
  volume       = {31},
  issn         = {1941-0468},
  doi          = {10.1109/TRO.2015.2463671},
  abstractnote = {This paper presents ORB-SLAM, a feature-based monocular simultaneous localization and mapping (SLAM) system that operates in real time, in small and large indoor and outdoor environments. The system is robust to severe motion clutter, allows wide baseline loop closing and relocalization, and includes full automatic initialization. Building on excellent algorithms of recent years, we designed from scratch a novel system that uses the same features for all SLAM tasks: tracking, mapping, relocalization, and loop closing. A survival of the fittest strategy that selects the points and keyframes of the reconstruction leads to excellent robustness and generates a compact and trackable map that only grows if the scene content changes, allowing lifelong operation. We present an exhaustive evaluation in 27 sequences from the most popular datasets. ORB-SLAM achieves unprecedented performance with respect to other state-of-the-art monocular SLAM approaches. For the benefit of the community, we make the source code public.},
  number       = {5},
  journal      = {IEEE Transactions on Robotics},
  author       = {Mur-Artal, Raul and Montiel, J. M. M. and Tardos, Juan D.},
  year         = {2015},
  month        = {Oct},
  pages        = {1147–1163}
}

 
 @inproceedings{Usenko_Engel_Stuckler_Cremers_2016,
  address      = {Stockholm},
  title        = {Direct visual-inertial odometry with stereo cameras},
  isbn         = {978-1-4673-8026-3},
  url          = {http://ieeexplore.ieee.org/document/7487335/},
  doi          = {10.1109/ICRA.2016.7487335},
  abstractnote = {We propose a novel direct visual-inertial odometry method for stereo cameras. Camera pose, velocity and IMU biases are simultaneously estimated by minimizing a combined photometric and inertial energy functional. This allows us to exploit the complementary nature of vision and inertial data. At the same time, and in contrast to all existing visual-inertial methods, our approach is fully direct: geometry is estimated in the form of semi-dense depth maps instead of manually designed sparse keypoints. Depth information is obtained both from static stereo – relating the ﬁxed-baseline images of the stereo camera – and temporal stereo – relating images from the same camera, taken at different points in time. We show that our method outperforms not only vision-only or loosely coupled approaches, but also can achieve more accurate results than state-of-the-art keypoint-based methods on different datasets, including rapid motion and signiﬁcant illumination changes. In addition, our method provides high-ﬁdelity semi-dense, metric reconstructions of the environment, and runs in real-time on a CPU.},
  booktitle    = {2016 IEEE International Conference on Robotics and Automation (ICRA)},
  publisher    = {IEEE},
  author       = {Usenko, Vladyslav and Engel, Jakob and Stuckler, Jorg and Cremers, Daniel},
  year         = {2016},
  month        = {May},
  pages        = {1885–1892},
  language     = {en}
}
 
  @inproceedings{Harmat_Sharf_Trentini_2012,
  address      = {Berlin, Heidelberg},
  title        = {Parallel Tracking and Mapping with Multiple Cameras on an Unmanned Aerial Vehicle},
  isbn         = {978-3-642-33509-9},
  abstractnote = {This paper presents the development and testing of a new multi-camera system designed for tracking and mapping aboard a small unmanned aerial vehicle. In order to be resistant to failure from loss of tracking, the large field of view cameras are positioned in such a way as to cover a large portion of the vehicle’s surroundings. The proposed algorithm is tested indoors, and it is found that the system achieves good performance. More importantly, it is shown that the system is resistant to failure by comparing it to a system with fewer cameras that loses tracking during certain portions of the test.},
  booktitle    = {Intelligent Robotics and Applications},
  publisher    = {Springer Berlin Heidelberg},
  author       = {Harmat, Adam and Sharf, Inna and Trentini, Michael},
  editor       = {Su, Chun-Yi and Rakheja, Subhash and Liu, Honghai},
  year         = {2012},
  pages        = {421–432}
}

@inproceedings{Leutenegger_Furgale_Rabaud_Chli_Konolige_Siegwart_2013,
  title        = {Keyframe-Based Visual-Inertial SLAM using Nonlinear Optimization},
  isbn         = {978-981-07-3937-9},
  url          = {http://www.roboticsproceedings.org/rss09/p37.pdf},
  doi          = {10.15607/RSS.2013.IX.037},
  abstractnote = {The fusion of visual and inertial cues has become popular in robotics due to the complementary nature of the two sensing modalities. While most fusion strategies to date rely on ﬁltering schemes, the visual robotics community has recently turned to non-linear optimization approaches for tasks such as visual Simultaneous Localization And Mapping (SLAM), following the discovery that this comes with signiﬁcant advantages in quality of performance and computational complexity. Following this trend, we present a novel approach to tightly integrate visual measurements with readings from an Inertial Measurement Unit (IMU) in SLAM. An IMU error term is integrated with the landmark reprojection error in a fully probabilistic manner, resulting to a joint non-linear cost function to be optimized. Employing the powerful concept of ‘keyframes’ we partially marginalize old states to maintain a bounded-sized optimization window, ensuring real-time operation. Comparing against both vision-only and loosely-coupled visual-inertial algorithms, our experiments conﬁrm the beneﬁts of tight fusion in terms of accuracy and robustness.},
  booktitle    = {Robotics: Science and Systems IX},
  publisher    = {Robotics: Science and Systems Foundation},
  author       = {Leutenegger, Stefan and Furgale, Paul and Rabaud, Vincent and Chli, Margarita and Konolige, Kurt and Siegwart, Roland},
  year         = {2013},
  month        = {Jun},
  language     = {en}
}

 @article{MurArtal_Tardos_2017,
  title        = {ORB-SLAM2: An Open-Source SLAM System for Monocular, Stereo, and RGB-D Cameras},
  volume       = {33},
  issn         = {1941-0468},
  doi          = {10.1109/TRO.2017.2705103},
  abstractnote = {We present ORB-SLAM2, a complete simultaneous localization and mapping (SLAM) system for monocular, stereo and RGB-D cameras, including map reuse, loop closing, and relocalization capabilities. The system works in real time on standard central processing units in a wide variety of environments from small hand-held indoors sequences, to drones flying in industrial environments and cars driving around a city. Our back-end, based on bundle adjustment with monocular and stereo observations, allows for accurate trajectory estimation with metric scale. Our system includes a lightweight localization mode that leverages visual odometry tracks for unmapped regions and matches with map points that allow for zero-drift localization. The evaluation on 29 popular public sequences shows that our method achieves state-of-the-art accuracy, being in most cases the most accurate SLAM solution. We publish the source code, not only for the benefit of the SLAM community, but with the aim of being an out-of-the-box SLAM solution for researchers in other fields.},
  number       = {5},
  journal      = {IEEE Transactions on Robotics},
  author       = {Mur-Artal, Raúl and Tardós, Juan D.},
  year         = {2017},
  month        = {Oct},
  pages        = {1255–1262}
}
 
 
 @article{Hulik_Spanel_Smrz_Materna_2014,
  title        = {Continuous plane detection in point-cloud data based on 3D Hough Transform},
  volume       = {25},
  issn         = {10473203},
  doi          = {10.1016/j.jvcir.2013.04.001},
  abstractnote = {This paper deals with shape extraction from depth images (point clouds) in the context of modern robotic vision systems. It presents various optimizations of the 3D Hough Transform used for plane extraction from point cloud data. Presented enhancements of standard methods address problems related to noisy data, high memory requirements for the parameter space and computational complexity of point accumulations. The realised robust plane detector beneﬁts from a continuous point cloud stream generated by a depth sensor over time. It is used for iterative reﬁnements of the results. The system is compared to a state-of-the-art RANSAC-based plane detector from the Point Cloud Library (PCL). Experimental results show that it overcomes the PCL alternative in the stability of plane detection and in the number of negative detections. This advantage is crucial for robotic applications, e.g., when a robot approaches a wall, it can be consistently recognized. The paper concludes with a discussion of further promising optimisation that will be implemented as a future step.},
  number       = {1},
  journal      = {Journal of Visual Communication and Image Representation},
  author       = {Hulik, Rostislav and Spanel, Michal and Smrz, Pavel and Materna, Zdenek},
  year         = {2014},
  month        = {Jan},
  pages        = {86–97},
  language     = {en}
}

 @article{Limberger_Oliveira_2015,
  title        = {Real-time detection of planar regions in unorganized point clouds},
  volume       = {48},
  issn         = {00313203},
  doi          = {10.1016/j.patcog.2014.12.020},
  abstractnote = {Automatic detection of planar regions in point clouds is an important step for many graphics, image processing, and computer vision applications. While laser scanners and digital photography have allowed us to capture increasingly larger datasets, previous techniques are computationally expensive, being unable to achieve real-time performance for datasets containing tens of thousands of points, even when detection is performed in a non-deterministic way. We present a deterministic technique for plane detection in unorganized point clouds whose cost is Oðn log nÞ in the number of input samples. It is based on an efﬁcient Hough-transform voting scheme and works by clustering approximately co-planar points and by casting votes for these clusters on a spherical accumulator using a trivariate Gaussian kernel. A comparison with competing techniques shows that our approach is considerably faster and scales signiﬁcantly better than previous ones, being the ﬁrst practical solution for deterministic plane detection in large unorganized point clouds.},
  number       = {6},
  journal      = {Pattern Recognition},
  author       = {Limberger, Frederico A. and Oliveira, Manuel M.},
  year         = {2015},
  month        = {Jun},
  pages        = {2043–2053},
  language     = {en},
  url          = {https://www.inf.ufrgs.br/~oliveira/pubs_files/HT3D/HT3D_page.html}
}
 
 @article{Durrant_Whyte_Bailey_2006,
  title        = {Simultaneous localization and mapping: part I},
  volume       = {13},
  issn         = {1558-223X},
  doi          = {10.1109/MRA.2006.1638022},
  abstractnote = {This paper describes the simultaneous localization and mapping (SLAM) problem and the essential methods for solving the SLAM problem and summarizes key implementations and demonstrations of the method. While there are still many practical issues to overcome, especially in more complex outdoor environments, the general SLAM method is now a well understood and established part of robotics. Another part of the tutorial summarized more recent works in addressing some of the remaining issues in SLAM, including computation, feature representation, and data association.},
  number       = {2},
  journal      = {IEEE Robotics Automation Magazine},
  author       = {Durrant-Whyte, H. and Bailey, T.},
  year         = {2006},
  month        = {Jun},
  pages        = {99–110}
}
 
 @article{Fischler1981RandomSC,
  title   = {Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography},
  author  = {Martin A. Fischler and Robert C. Bolles},
  journal = {Commun. ACM},
  year    = {1981},
  volume  = {24},
  pages   = {381-395}
}


@article{Yang_Forstner,
  title        = {Plane Detection in Point Cloud Data},
  abstractnote = {Plane detection is a prerequisite to a wide variety of vision tasks. RANdom SAmple Consensus (RANSAC) algorithm is widely used for plane detection in point cloud data. Minimum description length (MDL) principle is used to deal with several competing hypothesis. This paper presents a new approach to the plane detection by integrating RANSAC and MDL. The method could avoid detecting wrong planes due to the complex geometry of the 3D data. The paper tests the performance of proposed method on both synthetic and real data.},
  author       = {Yang, Michael Ying and Forstner, Wolfgang},
  pages        = {16},
  language     = {en}
}

 @article{Sun_Mordohai_2019,
  title        = {Oriented Point Sampling for Plane Detection in Unorganized Point Clouds},
  url          = {http://arxiv.org/abs/1905.02553},
  abstractnote = {Plane detection in 3D point clouds is a crucial pre-processing step for applications such as point cloud segmentation, semantic mapping and SLAM. In contrast to many recent plane detection methods that are only applicable on organized point clouds, our work is targeted to unorganized point clouds that do not permit a 2D parametrization. We compare three methods for detecting planes in point clouds efﬁciently. One is a novel method proposed in this paper that generates plane hypotheses by sampling from a set of points with estimated normals. We named this method Oriented Point Sampling (OPS) to contrast with more conventional techniques that require the sampling of three unoriented points to generate plane hypotheses. We also implemented an efﬁcient plane detection method based on local sampling of three unoriented points and compared it with OPS and the 3D-KHT algorithm, which is based on octrees, on the detection of planes on 10,000 point clouds from the SUN RGB-D dataset.},
  note         = {arXiv:1905.02553 [cs]},
  number       = {arXiv:1905.02553},
  publisher    = {arXiv},
  author       = {Sun, Bo and Mordohai, Philippos},
  year         = {2019},
  month        = {May},
  language     = {en},
  url          = {https://github.com/victor-amblard/OrientedPointSampling}
}

 @inbook{Oehler_Stueckler_Welle_Schulz_Behnke_2011,
  address      = {Berlin, Heidelberg},
  series       = {Lecture Notes in Computer Science},
  title        = {Efficient Multi-resolution Plane Segmentation of 3D Point Clouds},
  volume       = {7102},
  isbn         = {978-3-642-25488-8},
  url          = {http://link.springer.com/10.1007/978-3-642-25489-5_15},
  doi          = {10.1007/978-3-642-25489-5_15},
  abstractnote = {We present an efﬁcient multi-resolution approach to segment a 3D point cloud into planar components. In order to gain efﬁciency, we process large point clouds iteratively from coarse to ﬁne 3D resolutions: At each resolution, we rapidly extract surface normals to describe surface elements (surfels). We group surfels that cannot be associated with planes from coarser resolutions into coplanar clusters with the Hough transform. We then extract connected components on these clusters and determine a best plane ﬁt through RANSAC. Finally, we merge plane segments and reﬁne the segmentation on the ﬁnest resolution. In experiments, we demonstrate the efﬁciency and quality of our method and compare it to other state-of-the-art approaches.},
  booktitle    = {Intelligent Robotics and Applications},
  publisher    = {Springer Berlin Heidelberg},
  author       = {Oehler, Bastian and Stueckler, Joerg and Welle, Jochen and Schulz, Dirk and Behnke, Sven},
  editor       = {Jeschke, Sabina and Liu, Honghai and Schilberg, Daniel},
  year         = {2011},
  pages        = {145–156},
  collection   = {Lecture Notes in Computer Science},
  language     = {en}
}

@article{Adams_Bischof_1994,
  title        = {Seeded region growing},
  volume       = {16},
  issn         = {1939-3539},
  doi          = {10.1109/34.295913},
  abstractnote = {We present here a new algorithm for segmentation of intensity images which is robust, rapid, and free of tuning parameters. The method, however, requires the input of a number of seeds, either individual pixels or regions, which will control the formation of regions into which the image will be segmented. In this correspondence, we present the algorithm, discuss briefly its properties, and suggest two ways in which it can be employed, namely, by using manual seed selection or by automated procedures.<>},
  number       = {6},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  author       = {Adams, R. and Bischof, L.},
  year         = {1994},
  month        = {Jun},
  pages        = {641–647}
}

 @article{Hähnel_Burgard_Thrun_2003,
  title        = {Learning compact 3D models of indoor and outdoor environments with a mobile robot},
  volume       = {44},
  issn         = {09218890},
  doi          = {10.1016/S0921-8890(03)00007-1},
  abstractnote = {This paper presents an algorithm for full 3D shape reconstruction of indoor and outdoor environments with mobile robots. Data is acquired with laser range ﬁnders installed on a mobile robot. Our approach combines efﬁcient scan matching routines for robot pose estimation with an algorithm for approximating environments using ﬂat surfaces. On top of that, our approach includes a mesh simpliﬁcation technique to reduce the complexity of the resulting models. In extensive experiments, our method is shown to produce accurate models of indoor and outdoor environments that compare favorably to other methods.},
  number       = {1},
  journal      = {Robotics and Autonomous Systems},
  author       = {Hähnel, Dirk and Burgard, Wolfram and Thrun, Sebastian},
  year         = {2003},
  month        = {Jul},
  pages        = {15–27},
  language     = {en}
}

@article{Araújo_Oliveira_2020,
  title        = {A robust statistics approach for plane detection in unorganized point clouds},
  volume       = {100},
  issn         = {00313203},
  doi          = {10.1016/j.patcog.2019.107115},
  abstractnote = {Plane detection is a key component for many applications, such as industrial reverse engineering and self-driving cars. However, existing plane-detection techniques are sensitive to noise and to user-deﬁned parameters. We introduce a fast deterministic technique for plane detection in unorganized point clouds that is robust to noise and virtually independent of parameter tuning. It is based on a novel planarity test drawn from robust statistics and on a split and merge strategy. Its parameter values are automatically adjusted to ﬁt the local distribution of samples in the input dataset, thus lending to good reconstruction of even small planar regions. We demonstrate the eﬀectiveness of our solution on several real datasets, comparing its performance to state-of-art plane detection techniques, and showing that it achieves better accuracy, while still being one of the fastest.},
  journal      = {Pattern Recognition},
  author       = {Araújo, Abner M. C. and Oliveira, Manuel M.},
  year         = {2020},
  month        = {Apr},
  pages        = {107115},
  language     = {en}
}

@inproceedings{Sturm_Engelhard_Endres_Burgard_Cremers_2012,
  address      = {Vilamoura-Algarve, Portugal},
  title        = {A benchmark for the evaluation of RGB-D SLAM systems},
  isbn         = {978-1-4673-1736-8},
  url          = {http://ieeexplore.ieee.org/document/6385773/},
  doi          = {10.1109/IROS.2012.6385773},
  abstractnote = {In this paper, we present a novel benchmark for the evaluation of RGB-D SLAM systems. We recorded a large set of image sequences from a Microsoft Kinect with highly accurate and time-synchronized ground truth camera poses from a motion capture system. The sequences contain both the color and depth images in full sensor resolution (640 × 480) at video frame rate (30 Hz). The ground-truth trajectory was obtained from a motion-capture system with eight high-speed tracking cameras (100 Hz). The dataset consists of 39 sequences that were recorded in an ofﬁce environment and an industrial hall. The dataset covers a large variety of scenes and camera motions. We provide sequences for debugging with slow motions as well as longer trajectories with and without loop closures. Most sequences were recorded from a handheld Kinect with unconstrained 6-DOF motions but we also provide sequences from a Kinect mounted on a Pioneer 3 robot that was manually navigated through a cluttered indoor environment. To stimulate the comparison of different approaches, we provide automatic evaluation tools both for the evaluation of drift of visual odometry systems and the global pose error of SLAM systems. The benchmark website [1] contains all data, detailed descriptions of the scenes, speciﬁcations of the data formats, sample code, and evaluation tools.},
  booktitle    = {2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  publisher    = {IEEE},
  author       = {Sturm, Jrgen and Engelhard, Nikolas and Endres, Felix and Burgard, Wolfram and Cremers, Daniel},
  year         = {2012},
  month        = {Oct},
  pages        = {573–580},
  language     = {en}
}

@inproceedings{Handa_Whelan_McDonald_Davison_2014,
  address      = {Hong Kong, China},
  title        = {A benchmark for RGB-D visual odometry, 3D reconstruction and SLAM},
  isbn         = {978-1-4799-3685-4},
  url          = {http://ieeexplore.ieee.org/document/6907054/},
  doi          = {10.1109/ICRA.2014.6907054},
  abstractnote = {We introduce the Imperial College London and National University of Ireland Maynooth (ICL-NUIM) dataset for the evaluation of visual odometry, 3D reconstruction and SLAM algorithms that typically use RGB-D data. We present a collection of handheld RGB-D camera sequences within synthetically generated environments. RGB-D sequences with perfect ground truth poses are provided as well as a ground truth surface model that enables a method of quantitatively evaluating the ﬁnal map or surface reconstruction accuracy. Care has been taken to simulate typically observed real-world artefacts in the synthetic imagery by modelling sensor noise in both RGB and depth data. While this dataset is useful for the evaluation of visual odometry and SLAM trajectory estimation, our main focus is on providing a method to benchmark the surface reconstruction accuracy which to date has been missing in the RGB-D community despite the plethora of ground truth RGB-D datasets available.},
  booktitle    = {2014 IEEE International Conference on Robotics and Automation (ICRA)},
  publisher    = {IEEE},
  author       = {Handa, Ankur and Whelan, Thomas and McDonald, John and Davison, Andrew J.},
  year         = {2014},
  month        = {May},
  pages        = {1524–1531},
  language     = {en}
}

@inproceedings{armeni_cvpr16,
  title     = {3D Semantic Parsing of Large-Scale Indoor Spaces},
  author    = {Iro Armeni and Ozan Sener and Amir R. Zamir and Helen Jiang and Ioannis Brilakis and Martin Fischer and Silvio Savarese},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition},
  year      = {2016}
}

 @article{Labbé_Michaud_2019,
  title        = {RTAB-Map as an open-source lidar and visual simultaneous localization and mapping library for large-scale and long-term online operation: LABBÉ and MICHAUD},
  volume       = {36},
  issn         = {15564959},
  doi          = {10.1002/rob.21831},
  abstractnote = {Distributed as an open‐source library since 2013, real‐time appearance‐based mapping (RTAB‐Map) started as an appearance‐based loop closure detection approach with memory management to deal with large‐scale and long‐term online operation. It then grew to implement simultaneous localization and mapping (SLAM) on various robots and mobile platforms. As each application brings its own set of constraints on sensors, processing capabilities, and locomotion, it raises the question of which SLAM approach is the most appropriate to use in terms of cost, accuracy, computation power, and ease of integration. Since most of SLAM approaches are either visual‐ or lidar‐based, comparison is difficult. Therefore, we decided to extend RTAB‐Map to support both visual and lidar SLAM, providing in one package a tool allowing users to implement and compare a variety of 3D and 2D solutions for a wide range of applications with different robots and sensors. This paper presents this extended version of RTAB‐Map and its use in comparing, both quantitatively and qualitatively, a large selection of popular real‐world datasets (e.g., KITTI, EuRoC, TUM RGB‐D, MIT Stata Center on PR2 robot), outlining strengths, and limitations of visual and lidar SLAM configurations from a practical perspective for autonomous navigation applications.},
  number       = {2},
  journal      = {Journal of Field Robotics},
  author       = {Labbé, Mathieu and Michaud, François},
  year         = {2019},
  month        = {Mar},
  pages        = {416–446},
  language     = {en}
}

 @article{Vo_Truong-Hong_Laefer_Bertolotto_2015,
  title        = {Octree-based region growing for point cloud segmentation},
  volume       = {104},
  issn         = {09242716},
  doi          = {10.1016/j.isprsjprs.2015.01.011},
  abstractnote = {This paper introduces a novel, region-growing algorithm for the fast surface patch segmentation of three-dimensional point clouds of urban environments. The proposed algorithm is composed of two stages based on a coarse-to-ﬁne concept. First, a region-growing step is performed on an octree-based voxelized representation of the input point cloud to extract major (coarse) segments. The output is then passed through a reﬁnement process. As part of this, there are two competing factors related to voxel size selection. To balance the constraints, an adaptive octree is created in two stages. Empirical studies on real terrestrial and airborne laser scanning data for complex buildings and an urban setting show the proposed approach to be at least an order of magnitude faster when compared to a conventional region growing method and able to incorporate semantic-based feature criteria, while achieving precision, recall, and ﬁtness scores of at least 75% and as much as 95%.},
  journal      = {ISPRS Journal of Photogrammetry and Remote Sensing},
  author       = {Vo, Anh-Vu and Truong-Hong, Linh and Laefer, Debra F. and Bertolotto, Michela},
  year         = {2015},
  month        = {Jun},
  pages        = {88–100},
  language     = {en}
}

@article{2017arXiv170201105A,
  author        = {{Armeni}, I. and {Sax}, A. and {Zamir}, A.~R. and {Savarese}, S.
                   },
  title         = {{Joint 2D-3D-Semantic Data for Indoor Scene Understanding}},
  journal       = {ArXiv e-prints},
  archiveprefix = {arXiv},
  eprint        = {1702.01105},
  primaryclass  = {cs.CV},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
  year          = 2017,
  month         = feb,
  adsurl        = {http://adsabs.harvard.edu/abs/2017arXiv170201105A},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

 @inproceedings{Feng_Taguchi_Kamat_2014,
  address      = {Hong Kong, China},
  title        = {Fast plane extraction in organized point clouds using agglomerative hierarchical clustering},
  isbn         = {978-1-4799-3685-4},
  url          = {http://ieeexplore.ieee.org/document/6907776/},
  doi          = {10.1109/ICRA.2014.6907776},
  abstractnote = {Real-time plane extraction in 3D point clouds is crucial to many robotics applications. We present a novel algorithm for reliably detecting multiple planes in real time in organized point clouds obtained from devices such as Kinect sensors. By uniformly dividing such a point cloud into non-overlapping groups of points in the image space, we ﬁrst construct a graph whose node and edge represent a group of points and their neighborhood respectively. We then perform an agglomerative hierarchical clustering on this graph to systematically merge nodes belonging to the same plane until the plane ﬁtting mean squared error exceeds a threshold. Finally we reﬁne the extracted planes using pixel-wise region growing. Our experiments demonstrate that the proposed algorithm can reliably detect all major planes in the scene at a frame rate of more than 35Hz for 640x480 point clouds, which to the best of our knowledge is much faster than state-of-the-art algorithms.},
  booktitle    = {2014 IEEE International Conference on Robotics and Automation (ICRA)},
  publisher    = {IEEE},
  author       = {Feng, Chen and Taguchi, Yuichi and Kamat, Vineet R.},
  year         = {2014},
  month        = {May},
  pages        = {6218–6225},
  language     = {en}
}
 
@article{Proença_Gao_2018,
  title        = {Fast Cylinder and Plane Extraction from Depth Cameras for Visual Odometry},
  url          = {http://arxiv.org/abs/1803.02380},
  abstractnote = {This paper presents CAPE, a method to extract planes and cylinder segments from organized point clouds, which processes 640×480 depth images on a single CPU core at an average of 300 Hz, by operating on a grid of planar cells. While, compared to state-of-the-art plane extraction, the latency of CAPE is more consistent and 4-10 times faster, depending on the scene, we also demonstrate empirically that applying CAPE to visual odometry can improve trajectory estimation on scenes made of cylindrical surfaces (e.g. tunnels), whereas using a plane extraction approach that is not curve-aware deteriorates performance on these scenes.},
  note         = {number: arXiv:1803.02380
                  arXiv:1803.02380 [cs]},
  number       = {arXiv:1803.02380},
  publisher    = {arXiv},
  author       = {Proença, Pedro F. and Gao, Yang},
  year         = {2018},
  month        = {Jul},
  language     = {en}
}

@inproceedings{Mols_Li_Hanebeck_2020,
  address      = {Paris, France},
  title        = {Highly Parallelizable Plane Extraction for Organized Point Clouds Using Spherical Convex Hulls},
  isbn         = {978-1-72817-395-5},
  url          = {https://ieeexplore.ieee.org/document/9197139/},
  doi          = {10.1109/ICRA40945.2020.9197139},
  abstractnote = {We present a novel region growing algorithm for plane extraction of organized point clouds using the spherical convex hull. Instead of explicit plane parameterization, our approach interprets potential underlying planes as a series of geometric constraints on the sphere that are reﬁned during region growing. Unlike existing schemes relying on downsampling for sequential execution in real time, our approach enables pixelwise plane extraction that is highly parallelizable. We further test the proposed approach with a fully parallel implementation on a GPU. Evaluation based on public data sets has shown state-of-the-art extraction accuracy and superior speed compared to existing approaches, while guaranteeing realtime processing at full input resolution of a typical RGB-D camera.},
  booktitle    = {2020 IEEE International Conference on Robotics and Automation (ICRA)},
  publisher    = {IEEE},
  author       = {Mols, Hannes and Li, Kailai and Hanebeck, Uwe D.},
  year         = {2020},
  month        = {May},
  pages        = {7920–7926},
  language     = {en}
}

@article{Vera_Lucio_Fernandes_Velho_2018,
  title        = {Hough Transform for real-time plane detection in depth images},
  volume       = {103},
  issn         = {01678655},
  doi          = {10.1016/j.patrec.2017.12.027},
  abstractnote = {The automatic detection of planes in depth images plays an important role in computer vision. Plane detection from unorganized point clouds usually require complex data structures to pre-organize the points. On the other hand, existing detection approaches tailored to depth images use the structure of the image and the 2.5-D projection of the scene to simplify the task. However, they are sensitive to noise and to discontinuities caused by occlusion. We present a real-time deterministic technique for plane detection in depth images that uses an implicit quadtree to identify clusters of approximately coplanar points in the 2.5-D space. The detection is performed by an eﬃcient Hough-transform voting scheme that models the uncertainty associated with the best-ﬁtting plane with respect to each cluster as a Gaussian distribution. Experiments shows that our approach is fast, scalable, and robust even in the presence of noise, partial occlusion, and discontinuities.},
  journal      = {Pattern Recognition Letters},
  author       = {Vera, Eduardo and Lucio, Djalma and Fernandes, Leandro A.F. and Velho, Luiz},
  year         = {2018},
  month        = {Feb},
  pages        = {8–15},
  language     = {en}
}

@inproceedings{Roychoudhury_Missura_Bennewitz_2021,
  address      = {Prague, Czech Republic},
  title        = {Plane Segmentation Using Depth-Dependent Flood Fill},
  isbn         = {978-1-66541-714-3},
  url          = {https://ieeexplore.ieee.org/document/9635930/},
  doi          = {10.1109/IROS51168.2021.9635930},
  abstractnote = {The detection of planar surfaces in a point cloud is a popular technique for the extraction of drivable or walkable surfaces and for tabletop segmentation. Unfortunately, RGB-D sensors are quite noisy and provide incomplete data, which makes the extraction of surfaces more challenging. Also, it is desirable to process the point cloud data in real time, which at a rate of approximately 30 Hz, leaves only a small amount of computation time per frame. We have already developed a real time-capable plane segmentation method [1] that exploits the organized structure of RGB-D point clouds in order to implement a computationally efﬁcient region growing algorithm. It uses the point-plane distance to assign points to their segments rather than inherently unreliable surface normals. Now we are presenting an improvement where we adapt thresholds and other parameters of our algorithm to the measured depth in order to account for an increasing scatter of the points at larger distances from the camera. We estimate a minimum detectable plane size in pixels dependent on the measured depth. This enables us to stride in pixel coordinates with larger steps that are adaptive to the measured depth and to implement more robust sanity checks of depth-dependent size. Apart from a speed-up of the runtime of our algorithm, the segmentation quality also increased. We show a comparison between our improvement, our previous version, and other state-of-the-art methods evaluated on multiple commonly available datasets.},
  booktitle    = {2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  publisher    = {IEEE},
  author       = {Roychoudhury, Arindam and Missura, Marceli and Bennewitz, Maren},
  year         = {2021},
  month        = {Sep},
  pages        = {2210–2216},
  language     = {en}
}

@inproceedings{Liu_Yang_Ceylan_Yumer_Furukawa_2018,
  address      = {Salt Lake City, UT},
  title        = {PlaneNet: Piece-Wise Planar Reconstruction from a Single RGB Image},
  isbn         = {978-1-5386-6420-9},
  url          = {https://ieeexplore.ieee.org/document/8578371/},
  doi          = {10.1109/CVPR.2018.00273},
  abstractnote = {This paper proposes a deep neural network (DNN) for piece-wise planar depthmap reconstruction from a single RGB image. While DNNs have brought remarkable progress to single-image depth prediction, piece-wise planar depthmap reconstruction requires a structured geometry representation, and has been a difﬁcult task to master even for DNNs. The proposed end-to-end DNN learns to directly infer a set of plane parameters and corresponding plane segmentation masks from a single RGB image. We have generated more than 50,000 piece-wise planar depthmaps for training and testing from ScanNet, a largescale RGBD video database. Our qualitative and quantitative evaluations demonstrate that the proposed approach outperforms baseline methods in terms of both plane segmentation and depth estimation accuracy. To the best of our knowledge, this paper presents the ﬁrst end-to-end neural architecture for piece-wise planar reconstruction from a single RGB image. Code and data are available at https: //github.com/art-programmer/PlaneNet.},
  booktitle    = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  publisher    = {IEEE},
  author       = {Liu, Chen and Yang, Jimei and Ceylan, Duygu and Yumer, Ersin and Furukawa, Yasutaka},
  year         = {2018},
  month        = {Jun},
  pages        = {2579–2588},
  language     = {en}
}

@article{Xie_Shu_Rambach_Pagani_Stricker_2022,
  title        = {PlaneRecNet: Multi-Task Learning with Cross-Task Consistency for Piece-Wise Plane Detection and Reconstruction from a Single RGB Image},
  url          = {http://arxiv.org/abs/2110.11219},
  abstractnote = {Piece-wise 3D planar reconstruction provides holistic scene understanding of man-made environments, especially for indoor scenarios. Most recent approaches focused on improving the segmentation and reconstruction results by introducing advanced network architectures but overlooked the dual characteristics of piece-wise planes as objects and geometric models. Different from other existing approaches, we start from enforcing cross-task consistency for our multi-task convolutional neural network, PlaneRecNet, which integrates a single-stage instance segmentation network for piece-wise planar segmentation and a depth decoder to reconstruct the scene from a single RGB image. To achieve this, we introduce several novel loss functions (geometric constraint) that jointly improve the accuracy of piece-wise planar segmentation and depth estimation. Meanwhile, a novel Plane Prior Attention module is used to guide depth estimation with the awareness of plane instances. Exhaustive experiments are conducted in this work to validate the effectiveness and efficiency of our method.},
  note         = {number: arXiv:2110.11219
                  arXiv:2110.11219 [cs]},
  number       = {arXiv:2110.11219},
  publisher    = {arXiv},
  author       = {Xie, Yaxu and Shu, Fangwen and Rambach, Jason and Pagani, Alain and Stricker, Didier},
  year         = {2022},
  month        = {Jan},
  language     = {en}
}

@inproceedings{Liu_Kim_Gu_Furukawa_Kautz_2019,
  address      = {Long Beach, CA, USA},
  title        = {PlaneRCNN: 3D Plane Detection and Reconstruction From a Single Image},
  isbn         = {978-1-72813-293-8},
  url          = {https://ieeexplore.ieee.org/document/8953257/},
  doi          = {10.1109/CVPR.2019.00458},
  abstractnote = {This paper proposes a deep neural architecture, PlaneRCNN, that detects and reconstructs piecewise planar surfaces from a single RGB image. PlaneRCNN employs a variant of Mask R-CNN to detect planes with their plane parameters and segmentation masks. PlaneRCNN then jointly reﬁnes all the segmentation masks with a novel loss enforcing the consistency with a nearby view during training. The paper also presents a new benchmark with more ﬁne-grained plane segmentations in the ground-truth, in which, PlaneRCNN outperforms existing state-of-theart methods with signiﬁcant margins in the plane detection, segmentation, and reconstruction metrics. PlaneRCNN makes an important step towards robust plane extraction, which would have an immediate impact on a wide range of applications including Robotics, Augmented Reality, and Virtual Reality. Code and data are available at https://research.nvidia.com/publication/2019-06 PlaneRCNN.},
  booktitle    = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  publisher    = {IEEE},
  author       = {Liu, Chen and Kim, Kihwan and Gu, Jinwei and Furukawa, Yasutaka and Kautz, Jan},
  year         = {2019},
  month        = {Jun},
  pages        = {4445–4454},
  language     = {en}
}

 @article{Yang_Forstner,
  title        = {Plane Detection in Point Cloud Data},
  abstractnote = {Plane detection is a prerequisite to a wide variety of vision tasks. RANdom SAmple Consensus (RANSAC) algorithm is widely used for plane detection in point cloud data. Minimum description length (MDL) principle is used to deal with several competing hypothesis. This paper presents a new approach to the plane detection by integrating RANSAC and MDL. The method could avoid detecting wrong planes due to the complex geometry of the 3D data. The paper tests the performance of proposed method on both synthetic and real data.},
  author       = {Yang, Michael Ying and Forstner, Wolfgang},
  pages        = {16},
  language     = {en}
}

 @article{Ashraf_Ahmed_2017,
  title        = {FRANSAC: Fast RANdom Sample Consensus for 3D Plane Segmentation},
  volume       = {167},
  issn         = {09758887},
  doi          = {10.5120/ijca2017914558},
  abstractnote = {Scene analysis is a prior stage in many computer vision and robotics applications. Thanks to recent depth camera, we propose a fast plane segmentation approach for obstacle detection in indoor environments. The proposed method Fast RANdom Sample Consensus (FRANSAC) involves three steps: data input, data preprocessing and 3D RANSAC. Firstly, range data, obtained from 3D camera, is converted into 3D point clouds. Next, a preprocessing stage is introduced where a pass through and voxel grid filters are applied. Finally, planes are estimated using a modified 3D RANSAC. The experimental results demonstrate that our approach can segment planes and detect obstacles about 7 times faster than the standard RANSAC without losing the discriminative power.},
  number       = {13},
  journal      = {International Journal of Computer Applications},
  author       = {Ashraf, Ramy and Ahmed, Nawal},
  year         = {2017},
  month        = {Jun},
  pages        = {30–36},
  language     = {en}
}

 @inproceedings{Malek_Rahman_Yasiran_Jumaat_Jalil_2012,
  address      = {Langkawi, Kedah, Malaysia},
  title        = {Seed point selection for seed-based region growing in segmenting microcalcifications},
  isbn         = {978-1-4673-1582-1},
  url          = {http://ieeexplore.ieee.org/document/6396580/},
  doi          = {10.1109/ICSSBE.2012.6396580},
  abstractnote = {Seed-based region growing (SBRG) has been widely used as a segmentation method for medical images. The selection of initial seed point in SBRG is the crucial part before the segmentation process is carried out. Most of the region growing methods identify the seed point manually which involve human interaction and require prior information about the image. In this paper, an automated initial seed point selection for SBRG algorithm is proposed. The proposed method is tested on 50 mammogram images confirmed by a radiologist to consist microcalcifications. The performance is evaluated using Receiving Operator Curve (ROC) based on level of detection. Experimental results show that the method has successfully segmented the microcalcifications with 0.98 accuracy.},
  booktitle    = {2012 International Conference on Statistics in Science, Business and Engineering (ICSSBE)},
  publisher    = {IEEE},
  author       = {Malek, Aminah Abdul and Rahman, Wan Eny Zarina Wan Abdul and Yasiran, Siti Salmah and Jumaat, Abdul Kadir and Jalil, Ummu Mardhiah Abdul},
  year         = {2012},
  month        = {Sep},
  pages        = {1–5},
  language     = {en}
}

 @article{Labbé_Michaud_2019,
  title        = {RTAB-Map as an open-source lidar and visual simultaneous localization and mapping library for large-scale and long-term online operation: LABBÉ and MICHAUD},
  volume       = {36},
  issn         = {15564959},
  doi          = {10.1002/rob.21831},
  abstractnote = {Distributed as an open‐source library since 2013, real‐time appearance‐based mapping (RTAB‐Map) started as an appearance‐based loop closure detection approach with memory management to deal with large‐scale and long‐term online operation. It then grew to implement simultaneous localization and mapping (SLAM) on various robots and mobile platforms. As each application brings its own set of constraints on sensors, processing capabilities, and locomotion, it raises the question of which SLAM approach is the most appropriate to use in terms of cost, accuracy, computation power, and ease of integration. Since most of SLAM approaches are either visual‐ or lidar‐based, comparison is difficult. Therefore, we decided to extend RTAB‐Map to support both visual and lidar SLAM, providing in one package a tool allowing users to implement and compare a variety of 3D and 2D solutions for a wide range of applications with different robots and sensors. This paper presents this extended version of RTAB‐Map and its use in comparing, both quantitatively and qualitatively, a large selection of popular real‐world datasets (e.g., KITTI, EuRoC, TUM RGB‐D, MIT Stata Center on PR2 robot), outlining strengths, and limitations of visual and lidar SLAM configurations from a practical perspective for autonomous navigation applications.},
  number       = {2},
  journal      = {Journal of Field Robotics},
  author       = {Labbé, Mathieu and Michaud, François},
  year         = {2019},
  month        = {Mar},
  pages        = {416–446},
  language     = {en}
}

 @article{Borrmann_Elseberg_Lingemann_Nüchter_2011,
  title        = {The 3D Hough Transform for plane detection in point clouds: A review and a new accumulator design},
  volume       = {2},
  issn         = {2092-6731},
  doi          = {10.1007/3DRes.02(2011)3},
  abstractnote = {The Hough Transform is a well-known method for detecting parameterized objects. It is the de facto standard for detecting lines and circles in 2-dimensional data sets. For 3D it has attained little attention so far. Even for the 2D case high computational costs have lead to the development of numerous variations for the Hough Transform. In this article we evaluate different variants of the Hough Transform with respect to their applicability to detect planes in 3D point clouds reliably. Apart from computational costs, the main problem is the representation of the accumulator. Usual implementations favor geometrical objects with certain parameters due to uneven sampling of the parameter space. We present a novel approach to design the accumulator focusing on achieving the same size for each cell and compare it to existing designs.},
  number       = {2},
  journal      = {3D Research},
  author       = {Borrmann, Dorit and Elseberg, Jan and Lingemann, Kai and Nüchter, Andreas},
  year         = {2011},
  month        = {Jun},
  pages        = {3},
  language     = {en}
}

@article{Fernandes_Oliveira_2008,
  title        = {Real-time line detection through an improved Hough transform voting scheme},
  volume       = {41},
  issn         = {00313203},
  doi          = {10.1016/j.patcog.2007.04.003},
  abstractnote = {The Hough transform (HT) is a popular tool for line detection due to its robustness to noise and missing data. However, the computational cost associated to its voting scheme has prevented software implementations to achieve real-time performance, except for very small images. Many dedicated hardware designs have been proposed, but such architectures restrict the image sizes they can handle. We present an improved voting scheme for the HT that allows a software implementation to achieve real-time performance even on relatively large images. Our approach operates on clusters of approximately collinear pixels. For each cluster, votes are cast using an oriented elliptical-Gaussian kernel that models the uncertainty associated with the best-ﬁtting line with respect to the corresponding cluster. The proposed approach not only signiﬁcantly improves the performance of the voting scheme, but also produces a much cleaner voting map and makes the transform more robust to the detection of spurious lines.},
  number       = {1},
  journal      = {Pattern Recognition},
  author       = {Fernandes, Leandro A.F. and Oliveira, Manuel M.},
  year         = {2008},
  month        = {Jan},
  pages        = {299–314},
  language     = {en}
}
