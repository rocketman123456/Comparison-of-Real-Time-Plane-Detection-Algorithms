\documentclass[main.tex]{subfiles}
\begin{document}

\chapter{Evaluation}

In diesem kapitel werden zuvor ausgewählte algorithmen einheitlich verglichen und die resultierenden ergebnisse ausgewertet.

\section{Evaluation Protocol}
The goal of this work is to determine wether or not real-time, precise plane detection is realistic.
When selection an algorithm for said task a problem presents itself; Most algorithms had been tested on different, non-comparable conditions.
We therefore select a dataset on which all plane detection algorithms will be tested on to achieve comparability.
For the evaluation protocol itself, we took inspiration from the \textit{Results} Section of \citeauthor{Araújo_Oliveira_2020}s work on RSPD \cite{Araújo_Oliveira_2020}.  


\subsection{Metrics}
To quantitatively evaluate each algorithms performance, we calculate the precision, recall and the f1 score.
For that, we took inspiration from the work of \cite{Araújo_Oliveira_2020} and their approach of evaluation. 
First we regularize the original point cloud to reduce complexity and furthermore to avoid proximity bias, because of the inverse relationship
between distance to sensor and cloud density. This regularization is obtained through voxelization of the pointcloud.\\
With this voxelgrid we can now calculate corresponding sets of voxels for each point cloud representing a plane.
In the next step, we compare our planes from the ground truth with the planes obtained from an algorithm to obtain a list of corresponding pairs
of ground truth and found planes.\\
A grund truth plane $p_{gt_i}$ is marked as \textit{detected}, if any plane from the list of found planes achieves a voxel overlap of $\geq 50\%$.
With this list of correspondences, we calculate precision, recall and the f1-score as explained in the following.
For a given ground truth plane $p_{gt_j}$ and a corresponding found plane $p_{a_k}$ we sort a given voxel $v_i$ into the categories 
\textit{True Positive(TP), False Positive(FP), False Negative(FN), True Negative(TN)} as follows.
$$v_i \in p_{gt_j} \land v_i \in p_{a_k} \Rightarrow v_{i} \in TP$$
$$v_i \in p_{gt_j} \land v_i \notin p_{a_k} \Rightarrow v_{i} \in FN$$
$$v_i \notin p_{gt_j} \land v_i \in p_{a_k} \Rightarrow v_{i} \in FP$$
$$v_i \notin p_{gt_j} \land v_i \notin p_{a_k} \Rightarrow v_{i} \in TN$$

With those four rules, we can calculate the precision, recall and F1-score like this:
$$Precision = \frac{|TP|}{|TP|+|FP|}$$  
$$Recall = \frac{|TP|}{|TP|+|FN|}$$  
$$F1 = 2 \cdot\frac{Precision\cdot Recall}{Precision + Recall}$$  

Aside from the accuracy, we also need to compare the time each algorithm needs to find its respective set of planes.
For that, we measure the time spent in the plane detection phase, excluding any preprocessing or postprocessing steps, e.g. RSPD needs to calculate normals for each sample, 
if normal vectors are not included in the dataset. 
To measure the detection time, we simply log the exact times before and after calculations and write the differences to a file.\\
%TODO
\textcolor{red}{a variation would be interesting probably right? like time per plane, time per sample etc}



\subsection{Dataset}
To evaluate each plane detection algorithm on even grounds, we select the Stanford Large-Scale Indoor Spaces 3D Dataset(S3DIS). The Dataset had been recorded in three different buildings, 
which then were divided into six distinct areas. Those six areas include a total of 270 different types of rooms, e.g. offices, hallways, WCs and two auditoriums to name a few.\\
Each room has a complete unstructured point cloud in form of a list of XYZ values, as well as a list of annotated files representing semantically different objects that can be found in this point cloud 
of the room. 
Since our focus lies not on 3D semantic segmentation, we manually select planar regions using CloudCompare\footnote{https://cloudcompare.org/}, obtaining a list of sub-clouds.

%TODO
\textcolor{red}{insert statistic here}

\subsection{Real-Life Test}
Bei dem Live test wird im Gebäude der FIN ein Datensatz aufgenommen. Der Scan wird %TODO
\textcolor{red}{\$STUFF} beinhalten.
Zu diesem Scan gibt es keine Ground Truth, daher wird neben der Laufzeitanalyse lediglich eine qualitative Analyse der
Präzision vorgenommen.
Dafür überlagern wir die Punktwolke mit den gefundenen Ebenen.

\section{Results}
\subsection{Results Dataset}

Alle %TODO
\textcolor{red}{N} Sequenzen des Datensatzs wurde %TODO
\textcolor{red}{X} mal von jedem Algorithmus berechnet.

Hier sind die Ergebnisse:

\subsection{Results Real-Life Test}

Der FIN Datensatz wurde auch %TODO
\textcolor{red}{X} mal von jedem Datensatz berechnet.

Hier sind die Ergebnisse:


\end{document}